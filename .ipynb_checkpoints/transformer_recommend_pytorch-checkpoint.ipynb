{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac75e4fb-5718-46e0-94e6-a261eb3fdac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a2bc1-0825-4dc5-89ec-ba880e6724bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перевод на GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = x.to(device)\n",
    "x = torch.tensor([1.0, 2.0, 3.0], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf810b6-5a83-404c-bba8-034f3063d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Полносвязный слой (Linear) выполняет линейное преобразование входных данных.\n",
    "linear_layer = nn.Linear(in_features=128, out_features=64, bias=True)\n",
    "x = torch.randn(32, 128)  # [batch_size, in_features]\n",
    "linear_output = linear_layer(x)  # [32, 64]\n",
    "\n",
    "# Свёрточный слой (Conv2D) используется для обработки изображений и извлечения признаков.\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "x = torch.randn(1, 3, 32, 32)  # [batch_size, in_channels, height, width]\n",
    "conv_output = conv_layer(x)  # [1, 16, 32, 32]\n",
    "\n",
    "# Эмбеддинги (Embedding) позволяют представить категориальные признаки в виде плотных векторов.\n",
    "embedding_layer = nn.Embedding(num_embeddings=1000, embedding_dim=128)\n",
    "tokens = torch.randint(0, 1000, (4, 10))  # [batch, sequence_length]\n",
    "embedding_output = embedding_layer(tokens)  # [4, 10, 128]\n",
    "\n",
    "# Self-Attention (MultiheadAttention) используется в трансформерах для внимания к разным частям входных данных.\n",
    "multihead_attention = nn.MultiheadAttention(embed_dim=128, num_heads=8, dropout=0.1, batch_first=True)\n",
    "x = torch.randn(5, 10, 128)  # [batch, sequence_length, embed_dim]\n",
    "attn_output, _ = multihead_attention(x, x, x)  # [5, 10, 128]\n",
    "\n",
    "# Нормализация данных для ускорения обучения.\n",
    "layer_norm = nn.LayerNorm(normalized_shape=128)  # Нормализация по последней размерности\n",
    "batch_norm = nn.BatchNorm1d(num_features=128)  # Нормализация по batch_size\n",
    "x = torch.randn(32, 128)\n",
    "layer_norm_output = layer_norm(x)  # [32, 128]\n",
    "batch_norm_output = batch_norm(x)  # [32, 128]\n",
    "\n",
    "# Dropout - метод регуляризации, обнуляет случайные нейроны с вероятностью p.\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "dropout_output = dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c1477-83c4-43f8-9021-53d56e431838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание случайных тензоров с различными распределениями.\n",
    "rand_tensor = torch.rand(3, 3)  # Равномерное распределение [0, 1]\n",
    "randn_tensor = torch.randn(3, 3)  # Нормальное распределение [-1, 1]\n",
    "zeros_tensor = torch.zeros(3, 3)  # Заполнено нулями\n",
    "ones_tensor = torch.ones(3, 3)  # Заполнено единицами\n",
    "eye_tensor = torch.eye(3)  # Единичная матрица\n",
    "arange_tensor = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]\n",
    "linspace_tensor = torch.linspace(0, 10, 5)  # [0, 2.5, 5, 7.5, 10]\n",
    "\n",
    "# Операции с тензорами\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "sum_x = torch.sum(x)  # Сумма всех элементов\n",
    "mean_x = torch.mean(x.float())  # Среднее значение\n",
    "max_x, indices = torch.max(x, dim=1)  # Максимумы по каждой строке\n",
    "min_x = torch.min(x)  # Минимальное значение\n",
    "\n",
    "# Матричные операции\n",
    "mat1 = torch.randn(2, 3)\n",
    "mat2 = torch.randn(3, 4)\n",
    "matmul_result = torch.matmul(mat1, mat2)  # Перемножение матриц\n",
    "\n",
    "# Изменение формы тензоров\n",
    "x = torch.randn(4, 5)\n",
    "sliced_x = x[:, :3]  # Взять первые 3 столбца\n",
    "reshaped_x = x.reshape(2, 10)  # Изменение формы тензора\n",
    "unsqueezed_x = x.unsqueeze(0)  # Добавление оси\n",
    "squeezed_x = unsqueezed_x.squeeze()  # Удаление оси\n",
    "\n",
    "# Перестановка осей\n",
    "x = torch.randn(2, 3, 4)\n",
    "transposed_x = x.transpose(1, 2)  # Меняет оси местами\n",
    "permuted_x = x.permute(2, 0, 1)  # Полная смена порядка осей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc466af8-2335-401d-ac32-5fedbb5d0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorDataset позволяет объединить несколько тензоров в один датасет для удобного использования с DataLoader.\n",
    "x_data = torch.randn(100, 10)\n",
    "y_data = torch.randint(0, 2, (100,))\n",
    "dataset = TensorDataset(x_data, y_data)\n",
    "\n",
    "# Доступ к элементу:\n",
    "first_sample = dataset[0]  # Кортеж (x, y)\n",
    "\n",
    "# Создание DataLoader для обучения модели.\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7492aff-c77d-4a18-a3cd-06809b03fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, value, key, query):\n",
    "        attention = self.attention(query, key, value)[0]\n",
    "        x = self.norm1(attention + query)\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(forward + x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CrossInteractionAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(CrossInteractionAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "    \n",
    "    def forward(self, user_embed, session_embed):\n",
    "        attn_output, _ = self.attention(user_embed.unsqueeze(1), session_embed.unsqueeze(1), session_embed.unsqueeze(1))\n",
    "        return self.norm(attn_output.squeeze(1) + user_embed)\n",
    "\n",
    "\n",
    "class RecommenderModel(nn.Module):\n",
    "    def __init__(self, num_items, num_users, embed_size, num_heads, dropout, forward_expansion, num_layers, num_user_features, num_session_features):\n",
    "        super().__init__()\n",
    "        self.item_embedding = nn.Embedding(num_items, embed_size)\n",
    "        self.user_embedding = nn.Embedding(num_users, embed_size)\n",
    "        self.position_embedding = nn.Embedding(100, embed_size)\n",
    "        self.user_info_mlp = nn.Sequential(\n",
    "            nn.Linear(num_user_features, embed_size), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(embed_size, embed_size)\n",
    "        )\n",
    "        self.session_info_mlp = nn.Sequential(\n",
    "            nn.Linear(num_session_features, embed_size), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(embed_size, embed_size)\n",
    "        )\n",
    "        self.attention_pooling = nn.Linear(embed_size, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, num_heads, dropout, forward_expansion) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.cross_attention = CrossInteractionAttention(embed_size, num_heads)\n",
    "    \n",
    "    def forward(self, session_items, positions, user_ids, candidates, user_features, session_features):\n",
    "        item_embed = self.item_embedding(session_items)\n",
    "        pos_embed = self.position_embedding(positions)\n",
    "        x = self.dropout(item_embed + pos_embed)\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x)\n",
    "\n",
    "        attn_weights = torch.softmax(self.attention_pooling(x), dim=1)\n",
    "        session_representation = (attn_weights * x).sum(dim=1)\n",
    "        \n",
    "        session_embed = session_representation + self.session_info_mlp(session_features)\n",
    "        user_embed = self.user_embedding(user_ids) + self.user_info_mlp(user_features)\n",
    "        user_embed = self.cross_attention(user_embed, session_embed)\n",
    "        combined_representation = session_embed + user_embed\n",
    "\n",
    "        # Вариативно\n",
    "        # session_info_embed = self.session_info_mlp(session_info)\n",
    "        # user_embed = self.user_embedding(user_ids)\n",
    "        # user_info_embed = self.user_info_mlp(user_info)\n",
    "        # user_embed = self.cross_attention(user_embed, session_representation)\n",
    "        # combined_representation = session_representation + session_info_embed + user_embed + user_info_embed\n",
    "        \n",
    "        candidate_embeddings = self.item_embedding(candidates)\n",
    "        \n",
    "        scores = torch.matmul(combined_representation.unsqueeze(1), candidate_embeddings.transpose(1, 2)).squeeze(1)\n",
    "        return scores\n",
    "    \n",
    "    def recommend(self, session_items, positions, user_ids, candidates, session_info, user_info):\n",
    "        with torch.no_grad():\n",
    "            scores = self.forward(session_items, positions, user_ids, candidates, session_info, user_info)\n",
    "            return torch.topk(scores, k=5, dim=1).indices\n",
    "\n",
    "# Гиперпараметры\n",
    "num_items, num_users = 10000, 1000\n",
    "embed_size, num_heads, dropout, forward_expansion, num_layers = 128, 8, 0.1, 4, 2\n",
    "num_user_features, num_session_features = 10, 5\n",
    "num_samples = 1000\n",
    "\n",
    "# Искусственные данные\n",
    "session_items = torch.randint(0, num_items, (num_samples, 10))\n",
    "positions = torch.arange(10).repeat(num_samples, 1)\n",
    "session_features = torch.randn(num_samples, num_session_features)\n",
    "\n",
    "user_ids = torch.randint(0, num_users, (num_samples,))\n",
    "user_features = torch.randn(num_samples, num_user_features)\n",
    "\n",
    "candidates = torch.randint(0, num_items, (num_samples, 5))\n",
    "targets = torch.randint(0, 5, (num_samples,))\n",
    "\n",
    "dataset = TensorDataset(session_items, positions, user_ids, candidates, targets, user_features, session_features)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "model = RecommenderModel(num_items, num_users, embed_size, num_heads, dropout, forward_expansion, num_layers, num_user_features, num_session_features)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_fn, epochs=100):\n",
    "\n",
    "    def softmax_topk_loss(scores, targets, k=5):\n",
    "        \"\"\"\n",
    "        Softmax Loss по top-k кандидатов.\n",
    "        \"\"\"\n",
    "        topk_scores, topk_indices = torch.topk(scores, k, dim=1)  \n",
    "        new_targets = torch.full_like(targets, fill_value=-1)\n",
    "    \n",
    "        for i in range(targets.shape[0]):\n",
    "            if targets[i] in topk_indices[i]:\n",
    "                new_targets[i] = (topk_indices[i] == targets[i]).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "        mask = new_targets != -1\n",
    "        filtered_scores = topk_scores[mask]\n",
    "        filtered_targets = new_targets[mask]\n",
    "    \n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        return loss_fn(filtered_scores, filtered_targets) if filtered_scores.shape[0] > 0 else torch.tensor(0.0, requires_grad=True)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for session_items, positions, user_ids, candidates, targets, user_features, session_features in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(session_items, positions, user_ids, candidates, user_features, session_features)\n",
    "            loss = softmax_topk_loss(scores, targets, k=5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Запуск обучения\n",
    "train(model, train_loader, optimizer, loss_fn, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce8854-68ac-4c63-a1a5-bf661ddb938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# В forward перед return для TripletMarginLoss\n",
    "if negative_candidates is not None:\n",
    "    negative_embeddings = self.item_embedding(negative_candidates)\n",
    "    return scores, combined_representation, candidate_embeddings, negative_embeddings\n",
    "\n",
    "loss_fn = nn.TripletMarginLoss(margin=1.0)\n",
    "for session_items, positions, user_ids, candidates, negative_candidates, session_info, user_info in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    scores, user_repr, positive_repr, negative_repr = model(session_items, positions, user_ids, candidates, session_info, user_info, negative_candidates)\n",
    "    \n",
    "    loss = loss_fn(user_repr, positive_repr, negative_repr)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e66d74-07f0-42cf-a6e7-15d450bb1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pos_scores, neg_scores):\n",
    "        \"\"\"\n",
    "        pos_scores: (batch_size, 1) - предсказания для позитивных айтемов\n",
    "        neg_scores: (batch_size, num_neg) - предсказания для негативных айтемов\n",
    "        \"\"\"\n",
    "        loss = -torch.log(sigmoid(pos_scores.unsqueeze(1) - neg_scores)).mean()\n",
    "        return loss\n",
    "\n",
    "def forward(self, session_items, positions, user_ids, positive_candidates, session_info, user_info, negative_candidates):\n",
    "    \"\"\"\n",
    "    Возвращает предсказания для позитивных и негативных кандидатов\n",
    "    \"\"\"\n",
    "    # ...\n",
    "\n",
    "    pos_embeddings = self.item_embedding(positive_candidates)  # (batch_size, embed_size)\n",
    "    neg_embeddings = self.item_embedding(negative_candidates)  # (batch_size, num_neg, embed_size)\n",
    "\n",
    "    pos_scores = torch.matmul(combined_representation.unsqueeze(1), pos_embeddings.transpose(1, 2)).squeeze(1)\n",
    "    neg_scores = torch.matmul(combined_representation.unsqueeze(1), neg_embeddings.transpose(1, 2)).squeeze(1)\n",
    "\n",
    "    return pos_scores, neg_scores\n",
    "\n",
    "pos_scores, neg_scores = model(session_items, positions, user_ids, positive_candidates, session_info, user_info, negative_candidates)\n",
    "loss = bpr_loss(pos_scores, neg_scores)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3c22d-b32d-4368-afa7-9268c7d5e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(y_true, y_pred, k):\n",
    "    \"\"\" Вычисляет Normalized DCG (NDCG@K) \"\"\"\n",
    "\n",
    "    def dcg_at_k(relevance, k):\n",
    "        \"\"\" Вычисляет Discounted Cumulative Gain (DCG@K) \"\"\"\n",
    "        relevance = relevance[:k]\n",
    "        return np.sum(relevance / np.log2(np.arange(2, len(relevance) + 2)))\n",
    "    \n",
    "    order = np.argsort(y_pred)[::-1]  # Сортируем предсказания по убыванию\n",
    "    ideal_order = np.argsort(y_true)[::-1]  # Идеальный порядок (по истинной релевантности)\n",
    "\n",
    "    dcg = dcg_at_k(y_true[order], k)\n",
    "    idcg = dcg_at_k(y_true[ideal_order], k)  # DCG для идеального порядка\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "\n",
    "def map_at_k(y_true_list, y_pred_list, k):\n",
    "    \"\"\" Вычисляет Mean Average Precision (MAP@K) по всем пользователям \"\"\"\n",
    "\n",
    "    def average_precision_at_k(y_true, y_pred, k):\n",
    "        \"\"\" Вычисляет AP@K (Average Precision) \"\"\"\n",
    "        order = np.argsort(y_pred)[::-1]\n",
    "        relevance = y_true[order]\n",
    "        \n",
    "        num_relevant = 0\n",
    "        precision_sum = 0\n",
    "        \n",
    "        for i in range(k):\n",
    "            if relevance[i] == 1:\n",
    "                num_relevant += 1\n",
    "                precision_sum += num_relevant / (i + 1)  # Precision@i\n",
    "        return precision_sum / num_relevant if num_relevant > 0 else 0\n",
    "    return np.mean([average_precision_at_k(y_true, y_pred, k) for y_true, y_pred in zip(y_true_list, y_pred_list)])\n",
    "\n",
    "def recall_at_k(y_true, y_pred, k):\n",
    "    \"\"\" Вычисляет Recall@K \"\"\"\n",
    "    order = np.argsort(y_pred)[::-1][:k]  # Берем топ-K предсказаний\n",
    "    return np.sum(y_true[order]) / np.sum(y_true) if np.sum(y_true) > 0 else 0\n",
    "\n",
    "def evaluate_model(model, dataloader, k=10):\n",
    "    \"\"\" Оценивает модель по метрикам NDCG@K, MAP@K, Recall@K \"\"\"\n",
    "    all_ndcg, all_map, all_recall = [], [], []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            session_items, positions, user_ids, candidates, session_info, user_info, y_true = batch\n",
    "\n",
    "            scores = model(session_items, positions, user_ids, candidates, session_info, user_info)\n",
    "            scores = scores.cpu().numpy()\n",
    "            y_true = y_true.cpu().numpy()\n",
    "\n",
    "            for i in range(len(scores)):  # По каждому пользователю\n",
    "                all_ndcg.append(ndcg_at_k(y_true[i], scores[i], k))\n",
    "                all_map.append(average_precision_at_k(y_true[i], scores[i], k))\n",
    "                all_recall.append(recall_at_k(y_true[i], scores[i], k))\n",
    "\n",
    "    return {\n",
    "        \"NDCG@{}\".format(k): np.mean(all_ndcg),\n",
    "        \"MAP@{}\".format(k): np.mean(all_map),\n",
    "        \"Recall@{}\".format(k): np.mean(all_recall)\n",
    "    }\n",
    "\n",
    "# Должен быть даталоадер с батчами (session_items, positions, user_ids, candidates, session_info, user_info, y_true)\n",
    "# y_true - 0/1 релевантность кандидатов (1 - релевантный, 0 - нерелевантный)\n",
    "metrics = evaluate_model(model, test_dataloader, k=10)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d71aa5b-fb6e-404e-866d-d880b105b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "\n",
    "model = RecommenderModel(num_items, num_users, embed_size, num_heads, dropout, forward_expansion, num_layers, num_user_features, num_session_features)\n",
    "\n",
    "# Только если на CPU\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4645ee53-acb0-4997-9a4b-9a1510284f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(chosen_item, user_id):\n",
    "    # Получаем логи взаимодействий пользователя\n",
    "    user_interactions = interaction_logs[user_id]\n",
    "    \n",
    "    # Если пользователь кликнул на товар – даем награду 1.0\n",
    "    if chosen_item in user_interactions[\"clicked_items\"]:\n",
    "        return 1.0  \n",
    "    \n",
    "    # Если купил – еще более высокая награда\n",
    "    elif chosen_item in user_interactions[\"purchased_items\"]:\n",
    "        return 2.0  \n",
    "    \n",
    "    # Иначе штрафуем за плохую рекомендацию\n",
    "    return -0.5\n",
    "\n",
    "\n",
    "def get_user_context(user_id):\n",
    "    # Допустим, у нас есть словарь с контекстом пользователей\n",
    "    user_data = user_database[user_id]\n",
    "    \n",
    "    # Например, контекст – это демография, устройство и время суток\n",
    "    context = torch.tensor([\n",
    "        user_data[\"age\"] / 100,         # Возраст нормируем к 0-1\n",
    "        1 if user_data[\"device\"] == \"mobile\" else 0,  # 1, если мобильный\n",
    "        user_data[\"time_of_day\"] / 24   # Время суток, нормируем к 0-1\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    return context\n",
    "\n",
    "\n",
    "class ContextualBandit:\n",
    "    def __init__(self, num_actions, context_dim):\n",
    "        self.num_actions = num_actions\n",
    "        self.context_dim = context_dim\n",
    "        self.weights = torch.randn(num_actions, context_dim)\n",
    "\n",
    "    def select_action(self, context, candidates):\n",
    "        \"\"\"Выбираем товар из списка кандидатов\"\"\"\n",
    "        candidate_weights = self.weights[candidates]  # Берём веса только для кандидатов\n",
    "        scores = torch.matmul(candidate_weights, context)  # Оцениваем товары\n",
    "        return candidates[torch.argmax(scores).item()]  # Выбираем лучший\n",
    "\n",
    "    def update(self, context, action, reward, lr=0.1):\n",
    "        \"\"\"Обновляем веса для конкретного товара\"\"\"\n",
    "        pred_reward = torch.dot(self.weights[action], context)\n",
    "        self.weights[action] += lr * (reward - pred_reward) * context\n",
    "\n",
    "\n",
    "# 1. Получаем рекомендации из трансформера\n",
    "top_k_candidates = transformer_recommender.recommend(session_items, positions, user_ids, candidates, session_info, user_info)\n",
    "\n",
    "# 2. Выбираем оптимальный товар из кандидатов с помощью Bandit\n",
    "context = get_user_context(user_id)  # Например, демография, устройство и т.д.\n",
    "chosen_item = bandit.select_action(context, top_k_candidates)\n",
    "\n",
    "# 3. Показываем пользователю товар и получаем награду (например, клик)\n",
    "reward = get_reward(chosen_item, user_id)\n",
    "\n",
    "# 4. Обновляем веса Bandit\n",
    "bandit.update(context, chosen_item, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21439ddc-c227-4429-aff0-f9901ed35d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    def __init__(self, num_arms, epsilon=0.1):\n",
    "        self.num_arms = num_arms\n",
    "        self.epsilon = epsilon  # Доля случайных действий\n",
    "        self.q_values = np.zeros(num_arms)  # Средняя награда за каждый товар\n",
    "        self.counts = np.zeros(num_arms)  # Количество показов каждого товара\n",
    "    \n",
    "    def select_arm(self):\n",
    "        \"\"\"Выбор товара: либо лучший, либо случайный\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.num_arms)  # Исследуем случайный товар\n",
    "        return np.argmax(self.q_values)  # Выбираем товар с лучшей наградой\n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        \"\"\"Обновление оценок награды\"\"\"\n",
    "        self.counts[chosen_arm] += 1\n",
    "        self.q_values[chosen_arm] += (reward - self.q_values[chosen_arm]) / self.counts[chosen_arm]\n",
    "\n",
    "# === Используем бандита ===\n",
    "num_items = 10  # Допустим, у нас 10 товаров\n",
    "bandit = MultiArmedBandit(num_items)\n",
    "\n",
    "for _ in range(1000):  # Показываем товары 1000 раз\n",
    "    chosen_item = bandit.select_arm()  # Выбираем товар\n",
    "    reward = np.random.choice([0, 1], p=[0.8, 0.2])  # Случайная симуляция кликов (20% вероятность)\n",
    "    bandit.update(chosen_item, reward)\n",
    "\n",
    "print(\"Лучший товар по мнению бандита:\", np.argmax(bandit.q_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0119311-195a-473e-8902-bc7faae1968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB_Bandit:\n",
    "    def __init__(self, num_arms, c=2):\n",
    "        self.num_arms = num_arms\n",
    "        self.c = c  # Коэффициент доверительного интервала\n",
    "        self.q_values = np.zeros(num_arms)  # Средняя награда\n",
    "        self.counts = np.zeros(num_arms)  # Кол-во показов каждого товара\n",
    "        self.total_count = 0  # Общее число показов\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\"Выбираем товар на основе UCB\"\"\"\n",
    "        if np.any(self.counts == 0):\n",
    "            return np.argmin(self.counts)  # Сначала тестируем все товары хотя бы 1 раз\n",
    "        ucb_values = self.q_values + self.c * np.sqrt(np.log(self.total_count) / self.counts)\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        \"\"\"Обновляем веса на основе полученной награды\"\"\"\n",
    "        self.total_count += 1\n",
    "        self.counts[chosen_arm] += 1\n",
    "        self.q_values[chosen_arm] += (reward - self.q_values[chosen_arm]) / self.counts[chosen_arm]\n",
    "\n",
    "# === Используем UCB Bandit ===\n",
    "num_items = 10\n",
    "bandit = UCB_Bandit(num_items)\n",
    "\n",
    "for _ in range(1000):  \n",
    "    chosen_item = bandit.select_arm()\n",
    "    reward = np.random.choice([0, 1], p=[0.7, 0.3])  # 30% вероятность клика\n",
    "    bandit.update(chosen_item, reward)\n",
    "\n",
    "print(\"Лучший товар по мнению UCB:\", np.argmax(bandit.q_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92d757-8942-42d9-991e-cd47943f3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSamplingBandit:\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.successes = np.zeros(num_arms)  # Число успешных кликов\n",
    "        self.failures = np.zeros(num_arms)  # Число отказов\n",
    "\n",
    "    def select_arm(self):\n",
    "        \"\"\"Выбираем товар, сэмплируя из бета-распределения\"\"\"\n",
    "        beta_samples = np.random.beta(self.successes + 1, self.failures + 1)\n",
    "        return np.argmax(beta_samples)\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        \"\"\"Обновляем вероятностную модель\"\"\"\n",
    "        if reward == 1:\n",
    "            self.successes[chosen_arm] += 1\n",
    "        else:\n",
    "            self.failures[chosen_arm] += 1\n",
    "\n",
    "# === Используем Thompson Sampling ===\n",
    "num_items = 10\n",
    "bandit = ThompsonSamplingBandit(num_items)\n",
    "\n",
    "for _ in range(1000):  \n",
    "    chosen_item = bandit.select_arm()\n",
    "    reward = np.random.choice([0, 1], p=[0.6, 0.4])  # 40% вероятность клика\n",
    "    bandit.update(chosen_item, reward)\n",
    "\n",
    "print(\"Лучший товар по мнению Thompson Sampling:\", np.argmax(bandit.successes / (bandit.successes + bandit.failures)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
